{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose(object):\n",
    "\tdef __init__(self, transform_data, transform_target):\n",
    "\t\tself.transform_data = transform_data\n",
    "\t\tself.transform_target = transform_target\n",
    "\n",
    "\tdef __call__(self, image, target):\n",
    "\t\tfor t in self.transform_data:\n",
    "\t\t\timage = t(image)\n",
    "\n",
    "\t\tfor t in self.transform_target:\n",
    "\t\t\ttarget = t(target)\n",
    "\t\t\t\n",
    "\t\treturn image, target\n",
    "\t\n",
    "class NonZeroToOne(object):\n",
    "\tdef __call__(self, tensor):\n",
    "\t\treturn torch.where(tensor !=0, torch.tensor(1), tensor)\n",
    "\t\n",
    "transform_target = []\n",
    "transform_target.append(transforms.Resize((224,224)))\n",
    "transform_target.append(transforms.ToTensor())\n",
    "transform_target.append(NonZeroToOne())\n",
    "transform_data = []\n",
    "transform_data.append(transforms.Resize((224,224)))\n",
    "transform_data.append(transforms.ToTensor())\n",
    "transform = Compose(transform_data, transform_target)\n",
    "\n",
    "\n",
    "VOC2012_train = torchvision.datasets.VOCSegmentation('./data', image_set=\"train\",transforms=transform)\n",
    "VOC2012_test = torchvision.datasets.VOCSegmentation('./data', image_set=\"val\", transforms=transform)\n",
    "\n",
    "batch_size = 16\n",
    "trainloader = DataLoader(VOC2012_train, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(VOC2012_train, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1464\n",
      "torch.Size([1, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([1, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for i in range(200,300):\n",
    "#    for j in range(200,300):\n",
    "#        print((VOC2012_train[100][1][0][i][j].type(torch.int32)).item(),end=\"\")\n",
    "#    print(\"\")\n",
    "print(len(VOC2012_train))\n",
    "\n",
    "print(VOC2012_train[0][1].shape)\n",
    "print(VOC2012_train[0][0].shape)\n",
    "print(VOC2012_test[0][1].shape)\n",
    "print(VOC2012_test[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "\n",
    "https://lee-jaewon.github.io/deep_learning_study/FCN/\n",
    "\n",
    "https://medium.com/@msmapark2/fcn-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-fully-convolutional-networks-for-semantic-segmentation-81f016d76204\n",
    "\n",
    "https://gaussian37.github.io/vision-segmentation-fcn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input : (3, 224, 224)\n",
    "# output_layer1 : (64, 112, 112)\n",
    "# output_layer2 : (128, 56, 56)\n",
    "# output_layer3 : (256, 28, 28)\n",
    "# output_layer4 : (512, 14, 14)\n",
    "# output_layer5 : (1024, 7, 7)\n",
    "\n",
    "# output_fc6 : (4096, 7, 7)\n",
    "# output_fc7 : (1000, 7, 7)\n",
    "# output_fc8 : (10, 7, 7)\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(weights=True)\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FCN, self).__init__()\n",
    "        self.conv1_1=nn.Conv2d(in_channels=3,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.relu1_1=nn.ReLU()\n",
    "        self.conv1_2=nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1)\n",
    "        self.relu1_2=nn.ReLU()\n",
    "        self.pool1=nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2_1=nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.relu2_1=nn.ReLU()\n",
    "        self.conv2_2=nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1)\n",
    "        self.relu2_2=nn.ReLU()\n",
    "        self.pool2=nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3_1=nn.Conv2d(in_channels=128,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.relu3_1=nn.ReLU()\n",
    "        self.conv3_2=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.relu3_2=nn.ReLU()\n",
    "        self.conv3_3=nn.Conv2d(in_channels=256,out_channels=256,kernel_size=3,padding=1)\n",
    "        self.relu3_3=nn.ReLU()\n",
    "        self.pool3=nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv4_1=nn.Conv2d(in_channels=256,out_channels=512,kernel_size=3,padding=1)\n",
    "        self.relu4_1=nn.ReLU()\n",
    "        self.conv4_2=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1)\n",
    "        self.relu4_2=nn.ReLU()\n",
    "        self.conv4_3=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1)\n",
    "        self.relu4_3=nn.ReLU()\n",
    "        self.pool4=nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv5_1=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1)\n",
    "        self.relu5_1=nn.ReLU()\n",
    "        self.conv5_2=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1)\n",
    "        self.relu5_2=nn.ReLU()\n",
    "        self.conv5_3=nn.Conv2d(in_channels=512,out_channels=512,kernel_size=3,padding=1)\n",
    "        self.relu5_3=nn.ReLU()\n",
    "        self.pool5=nn.MaxPool2d(2)\n",
    "        \n",
    "        self.score_pool3=nn.Conv2d(in_channels=256,out_channels=num_classes,kernel_size=1,stride=1,padding=0)\n",
    "    \n",
    "        self.score_pool4=nn.Conv2d(in_channels=512,out_channels=num_classes,kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.fc6=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512,out_channels=4096,kernel_size=1,stride=1,padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d()\n",
    "        )\n",
    "        self.fc7=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4096,out_channels=4096,kernel_size=1,stride=1,padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d()\n",
    "        )\n",
    "        self.score_fr=nn.Conv2d(in_channels=4096,out_channels=num_classes,kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        \n",
    "        self.x8_up=nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=2*8,stride=8)\n",
    "        self.x2_up=nn.ConvTranspose2d(in_channels=num_classes,out_channels=num_classes,kernel_size=2*2,stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.relu1_1(self.conv1_1(x))\n",
    "        x=self.relu1_2(self.conv1_2(x))\n",
    "        x=self.pool1(x)\n",
    "\n",
    "        x=self.relu2_1(self.conv2_1(x))\n",
    "        x=self.relu2_2(self.conv2_2(x))\n",
    "        x=self.pool2(x)\n",
    "\n",
    "        x=self.relu3_1(self.conv3_1(x))\n",
    "        x=self.relu3_2(self.conv3_2(x))\n",
    "        x=self.relu3_3(self.conv3_3(x))\n",
    "        x=self.pool3(x)\n",
    "        pool3_pred=self.score_pool3(x)\n",
    "\n",
    "        x=self.relu4_1(self.conv4_1(x))\n",
    "        x=self.relu4_2(self.conv4_2(x))\n",
    "        x=self.relu4_3(self.conv4_3(x))\n",
    "        x=self.pool4(x)\n",
    "        pool4_pred=self.score_pool4(x)\n",
    "\n",
    "        x=self.relu5_1(self.conv5_1(x))\n",
    "        x=self.relu5_2(self.conv5_2(x))\n",
    "        x=self.relu5_3(self.conv5_3(x))\n",
    "        x=self.pool5(x)\n",
    "\n",
    "        x=self.fc6(x)\n",
    "        x=self.fc7(x)\n",
    "        x=self.score_fr(x)\n",
    "        us=self.x2_up(x)\n",
    "        us=us[:,:,1:-1,1:-1]\n",
    "    \n",
    "        us_with_pool4_prediction=us+pool4_pred\n",
    "\n",
    "        us2=self.x2_up(us_with_pool4_prediction)\n",
    "        us2=us2[:,:,1:-1,1:-1]\n",
    "        us2_with_pool3_prediction=us2+pool3_pred\n",
    "        FCN_8s=self.x8_up(us2_with_pool3_prediction)\n",
    "        FCN_8s=FCN_8s[:,:,4:-4,4:-4]\n",
    "        return FCN_8s\n",
    "    \n",
    "    def copy_params_from_vgg16(self, vgg16):\n",
    "        features=[\n",
    "            self.conv1_1, self.relu1_1,\n",
    "            self.conv1_2, self.relu1_2,\n",
    "            self.pool1,\n",
    "            self.conv2_1, self.relu2_1,\n",
    "\t\t\tself.conv2_2, self.relu2_2,\n",
    "            self.pool2,\n",
    "            self.conv3_1, self.relu3_1,\n",
    "\t\t\tself.conv3_2, self.relu3_2,\n",
    "\t\t\tself.conv3_3, self.relu3_3,\n",
    "            self.pool3,\n",
    "            self.conv4_1, self.relu4_1,\n",
    "\t\t\tself.conv4_2, self.relu4_2,\n",
    "\t\t\tself.conv4_3, self.relu4_3,\n",
    "            self.pool4,\n",
    "            self.conv5_1, self.relu5_1,\n",
    "\t\t\tself.conv5_2, self.relu5_2,\n",
    "\t\t\tself.conv5_3, self.relu5_3,\n",
    "            self.pool5\n",
    "        ]\n",
    "        for l1, l2 in zip(vgg16.features, features):\n",
    "            if isinstance(l1, nn.Conv2d) and isinstance(l2,nn.Conv2d):\n",
    "                assert l1.weight.size()==l2.weight.size()\n",
    "                assert l1.bias.size()==l2.bias.size()\n",
    "                l2.weight.data.copy_(l1.weight.data)\n",
    "                l2.bias.data.copy_(l1.bias.data)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vgg16 \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mvgg16(weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#learning_rate=1E-3\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m=\u001b[39mFCN(num_classes\u001b[39m=\u001b[39;49m\u001b[39m21\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mcopy_params_from_vgg16(vgg16)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#optim=torch.optim.Adam(params=model.parameters(),lr=learning_rate)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AIstudy/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/Desktop/AIstudy/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AIstudy/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/AIstudy/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1160\u001b[0m         device,\n\u001b[1;32m   1161\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1162\u001b[0m         non_blocking,\n\u001b[1;32m   1163\u001b[0m     )\n\u001b[1;32m   1164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "vgg16 = torchvision.models.vgg16(weights=True)\n",
    "\n",
    "#learning_rate=1E-3\n",
    "\n",
    "model=FCN(num_classes=21).to(device)\n",
    "model.copy_params_from_vgg16(vgg16)\n",
    "#optim=torch.optim.Adam(params=model.parameters(),lr=learning_rate)\n",
    "\n",
    "\n",
    "epoch=1\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "#print(len(trainloader.dataset))\n",
    "lr = 10**-1\n",
    "momentum = 0.9\n",
    "weight_decay = 2**-4\n",
    "optim = torch.optim.SGD(params=model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "#\n",
    "\n",
    "for epoch_cnt in range(epoch):\n",
    "    last_loss=0\n",
    "    for index, (data, target) in tqdm(enumerate(trainloader)):\n",
    "        #print(data.shape)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optim.zero_grad()\n",
    "        prediction=model(data)\n",
    "        #print(prediction.max())\n",
    "        target = torch.squeeze(target)\n",
    "        target = target.long()\n",
    "        loss=loss_fn(prediction,target)\n",
    "        loss.backward()\n",
    "        last_loss=last_loss+loss.item()\n",
    "        optim.step()\n",
    "    print(last_loss/len(trainloader))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.5599e-01,  4.0321e-01,  2.9290e-01,  ...,  2.9288e-01,\n",
      "           2.6524e-01,  3.2477e-01],\n",
      "         [ 3.4972e-01,  3.3274e-01,  2.8565e-01,  ...,  3.4133e-01,\n",
      "           3.5300e-01,  3.3788e-01],\n",
      "         [ 3.0424e-01,  3.2251e-01,  2.7778e-01,  ...,  2.1243e-01,\n",
      "           4.0355e-01,  3.5414e-01],\n",
      "         ...,\n",
      "         [ 3.7433e-01,  3.7313e-01,  2.8504e-01,  ...,  3.1121e-01,\n",
      "           4.0119e-01,  2.6791e-01],\n",
      "         [ 2.7598e-01,  3.4845e-01,  3.9136e-01,  ...,  3.4372e-01,\n",
      "           3.2231e-01,  3.0929e-01],\n",
      "         [ 4.4944e-01,  3.3752e-01,  3.0482e-01,  ...,  3.5178e-01,\n",
      "           2.6787e-01,  2.3787e-01]],\n",
      "\n",
      "        [[ 1.8478e-03, -1.0671e-01,  3.9923e-02,  ...,  3.0852e-02,\n",
      "           3.2677e-03,  3.8513e-02],\n",
      "         [ 5.1723e-03, -4.2358e-02, -2.9416e-02,  ..., -1.2927e-02,\n",
      "          -5.1456e-02, -1.6787e-02],\n",
      "         [ 3.6208e-02,  1.2849e-01,  4.3911e-02,  ...,  2.1054e-02,\n",
      "           4.9695e-02, -3.3475e-02],\n",
      "         ...,\n",
      "         [ 8.7353e-03,  1.0609e-01, -1.8969e-02,  ...,  1.3402e-01,\n",
      "           4.5714e-02, -9.1766e-02],\n",
      "         [-9.9840e-02, -5.6890e-03, -2.5027e-02,  ...,  1.0130e-01,\n",
      "          -5.8570e-02,  5.2368e-02],\n",
      "         [ 5.4386e-02, -5.6513e-03, -1.2556e-01,  ...,  4.3146e-02,\n",
      "           1.1508e-01,  7.3223e-02]],\n",
      "\n",
      "        [[-5.2041e-02, -5.5601e-02, -1.6097e-01,  ..., -3.9962e-02,\n",
      "          -1.0699e-01,  3.7499e-02],\n",
      "         [-1.2376e-01, -9.0087e-03, -9.8931e-02,  ..., -2.5362e-02,\n",
      "          -4.3614e-02,  3.2005e-04],\n",
      "         [-6.6756e-03,  5.0387e-02,  3.8214e-02,  ..., -3.6559e-02,\n",
      "           3.7089e-02, -4.1338e-02],\n",
      "         ...,\n",
      "         [-3.9543e-02, -1.0390e-01, -1.2310e-01,  ..., -5.7155e-02,\n",
      "           1.1349e-01, -3.1132e-02],\n",
      "         [ 1.1885e-02, -1.3775e-01, -7.9237e-02,  ..., -3.8471e-03,\n",
      "          -7.0980e-02, -2.2258e-02],\n",
      "         [ 4.0801e-02, -3.0605e-02, -7.4257e-02,  ..., -1.3430e-01,\n",
      "           6.8621e-03,  2.9752e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.6225e-02, -2.3343e-02,  9.9087e-03,  ..., -1.6613e-01,\n",
      "           8.4532e-02,  2.6434e-02],\n",
      "         [ 2.1949e-02, -1.1140e-01, -3.4508e-02,  ...,  9.8238e-02,\n",
      "          -1.4092e-02, -5.8113e-02],\n",
      "         [-3.2973e-02, -3.8967e-02, -1.6724e-03,  ..., -1.4854e-01,\n",
      "          -1.2293e-01, -4.3796e-02],\n",
      "         ...,\n",
      "         [-8.2512e-02, -1.0120e-01, -1.0749e-01,  ...,  4.4589e-02,\n",
      "          -4.0585e-02, -5.0266e-02],\n",
      "         [ 1.0347e-01,  4.1403e-02, -1.2753e-01,  ..., -6.5085e-02,\n",
      "          -2.0248e-02,  1.0882e-02],\n",
      "         [-1.5997e-01,  5.9525e-04, -1.2452e-01,  ...,  2.2854e-02,\n",
      "          -7.4573e-02,  1.7073e-02]],\n",
      "\n",
      "        [[-1.1665e-02,  1.0526e-01,  6.5942e-02,  ...,  2.9133e-02,\n",
      "          -6.0481e-02, -1.3938e-01],\n",
      "         [ 1.6475e-02, -6.3504e-02, -3.6522e-02,  ...,  5.1190e-02,\n",
      "          -3.4429e-02,  5.1633e-02],\n",
      "         [-3.8672e-02,  5.1069e-02,  5.3987e-02,  ...,  4.6849e-03,\n",
      "           3.5032e-02, -7.3963e-02],\n",
      "         ...,\n",
      "         [ 1.3973e-02, -1.3340e-03,  2.4709e-02,  ...,  3.4854e-02,\n",
      "          -4.3988e-02,  4.0543e-02],\n",
      "         [ 7.0470e-02, -5.8011e-02,  1.0564e-01,  ..., -1.6140e-02,\n",
      "          -4.3254e-02,  1.8105e-01],\n",
      "         [-2.1234e-03,  7.9849e-02,  4.8600e-02,  ..., -1.3040e-02,\n",
      "           4.0987e-02, -2.4319e-02]],\n",
      "\n",
      "        [[-1.1427e-01, -4.2581e-02, -8.9707e-02,  ..., -6.6571e-02,\n",
      "           4.4115e-02, -2.2710e-02],\n",
      "         [-7.4718e-02, -1.6073e-02, -1.0524e-01,  ..., -3.4817e-02,\n",
      "          -3.8940e-02, -3.6049e-02],\n",
      "         [ 4.3775e-02, -4.2931e-02,  5.9827e-03,  ...,  4.8064e-02,\n",
      "          -3.8388e-02, -7.9156e-02],\n",
      "         ...,\n",
      "         [-3.5914e-02, -9.5989e-02, -3.8930e-02,  ...,  8.6121e-02,\n",
      "          -7.8942e-02,  1.1952e-02],\n",
      "         [-6.0762e-02,  7.0198e-03, -5.0772e-02,  ..., -7.6987e-02,\n",
      "          -9.4745e-02,  5.2041e-02],\n",
      "         [ 6.2900e-02,  3.6911e-02, -8.5464e-02,  ...,  5.3704e-02,\n",
      "           3.6806e-02,  3.4114e-02]]])\n",
      "tensor(1.0654)\n",
      "torch.Size([224, 224])\n",
      "tensor(20)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cm' has no attribute 'get_cmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(prediction\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(prediction\u001b[39m.\u001b[39mmax())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m cmap \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49mcm\u001b[39m.\u001b[39;49mget_cmap(\u001b[39m'\u001b[39m\u001b[39mtab20\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m20\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m121\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hyunwook/Desktop/AIstudy/week6/week6.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cm' has no attribute 'get_cmap'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, (input,output) in enumerate(trainloader):\n",
    "        input,output=input.to(device), output.to(device)\n",
    "        prediction=model(input)\n",
    "\n",
    "        prediction=prediction[0].cpu()\n",
    "        print(prediction)\n",
    "        print(prediction.max())\n",
    "        prediction = torch.argmax(prediction, dim=0)\n",
    "        print(prediction.shape)\n",
    "        print(prediction.max())\n",
    "\n",
    "        cmap = plt.cm.get_cmap('tab20', 20)\n",
    "        plt.subplot(121)\n",
    "        for i in range(20):\n",
    "            plt.imshow(prediction, cmap=cmap, alpha=0.5)\n",
    "        plt.subplot(122)\n",
    "        for i in range(20):\n",
    "            plt.imshow(output.cpu()[0].squeeze(), cmap=cmap, alpha=0.5)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
